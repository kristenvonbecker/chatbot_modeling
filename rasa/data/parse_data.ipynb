{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "import ruamel.yaml\n",
    "yaml = ruamel.yaml.YAML()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# paths\n",
    "\n",
    "home = os.getenv(\"PROJ_HOME\")\n",
    "\n",
    "data_in_filepath = os.path.join(home, \"data/chatbot_knowledgebase/subject_matter/encyclopedia_britannica.csv\")\n",
    "subject_list_filepath = os.path.join(home, \"rasa/data/nlu/lookups/subjects.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# load encyclopedia data\n",
    "\n",
    "data = pd.read_csv(data_in_filepath,\n",
    "                   header=0,\n",
    "                   names=[\"Id\", \"Title\", \"Alt Title\", \"Field\", \"Is Person\", \"Text\"],\n",
    "                   index_col=0\n",
    "                   )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# write list of unique subjects to a txt file for lookup\n",
    "\n",
    "titles = data[\"Title\"]\n",
    "alt_titles = data[\"Alt Title\"].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "subjects = []\n",
    "for title, aliases in zip(titles, alt_titles):\n",
    "    these_titles = [title] + aliases\n",
    "    subjects += these_titles\n",
    "\n",
    "subjects = list(set(subjects))\n",
    "\n",
    "paren_pattern = r\"(\\(.*?\\))|(\\(.*)\"\n",
    "\n",
    "clean_subjects = []\n",
    "for subject in subjects:\n",
    "    this_subject = subject.split(\":\")[0]\n",
    "    this_subject = re.sub(paren_pattern, \"\", this_subject)\n",
    "    parts = this_subject.split(\".\")\n",
    "    num_parts = len(parts)\n",
    "    this_subject = \"\"\n",
    "    for count, part in enumerate(parts):\n",
    "        if len(part) <= 4 and count < num_parts - 1:\n",
    "            this_subject += part + \".\"\n",
    "        else:\n",
    "            this_subject += part\n",
    "            break\n",
    "    clean_subjects.append(this_subject.strip())\n",
    "\n",
    "with open(subject_list_filepath, \"w\") as txt_file:\n",
    "    for item in clean_subjects:\n",
    "        txt_file.write(f\"{item}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# generate annotated ask_for_explanation intents from items in subject_list\n",
    "\n",
    "non_anno_examples = []\n",
    "\n",
    "no_annotations_filepath = os.path.join(home, \"rasa/data/nlu/intents/ask_for_explanation/no_annotation.txt\")\n",
    "with open(no_annotations_filepath, \"r\") as file:\n",
    "    for line in file:\n",
    "        non_anno_examples.append(line.strip(\"\\n\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "templates = []\n",
    "\n",
    "templates_filepath = os.path.join(home, \"rasa/data/nlu/intents/ask_for_explanation/templates.txt\")\n",
    "with open(templates_filepath, \"r\") as file:\n",
    "    for line in file:\n",
    "        templates.append(line.strip(\"\\n\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# merge subject list and intent templates to form list of annotated ask_for_explanation intent exampes\n",
    "\n",
    "anno_pattern = \"(\\[.*?\\])\"\n",
    "\n",
    "num_templates = len(templates)\n",
    "\n",
    "def fill_template(filler, n):\n",
    "    template = templates[n % num_templates]\n",
    "    anno_subject = \"[\" + filler + \"]\"\n",
    "    match = re.search(anno_pattern, template)\n",
    "    filled = template.replace(match.group(0), anno_subject)\n",
    "    return filled\n",
    "\n",
    "intent_examples = non_anno_examples\n",
    "i = 0\n",
    "for subject in clean_subjects:\n",
    "    intent = fill_template(subject, i)\n",
    "    intent_examples.append(intent)\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "# write ask_for_explanation intent examples to a txt file\n",
    "\n",
    "intents_txt = os.path.join(home, \"rasa/data/nlu/intents/ask_for_explanation/examples.txt\")\n",
    "\n",
    "with open(intents_txt, \"w\") as txt_file:\n",
    "    for item in intent_examples:\n",
    "        txt_file.write(f\"{item}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# generate yaml file for intent ask_for_explanation\n",
    "\n",
    "intent_dict = {\n",
    "    \"version\": \"3.1\",\n",
    "    \"nlu\": [\n",
    "        {\n",
    "            \"intent\": \"ask_for_explanation\",\n",
    "            \"examples\": intent_examples\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "def literalize_list(v):\n",
    "    assert isinstance(v, list)\n",
    "    buf = io.StringIO()\n",
    "    yaml.dump(v, buf)\n",
    "    return ruamel.yaml.scalarstring.LiteralScalarString(buf.getvalue())\n",
    "\n",
    "def transform_value(d, key, transformation):\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            if k == key:\n",
    "                d[k] = transformation(v)\n",
    "            else:\n",
    "                transform_value(v, key, transformation)\n",
    "    elif isinstance(d, list):\n",
    "        for elem in d:\n",
    "            transform_value(elem, key, transformation)\n",
    "\n",
    "transform_value(intent_dict, 'examples', literalize_list)\n",
    "\n",
    "yaml_file = os.path.join(home, \"rasa/data/nlu/intent_ask_for_explanation.yml\")\n",
    "\n",
    "with open(yaml_file, \"w\") as file:\n",
    "    yaml.dump(intent_dict, file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
